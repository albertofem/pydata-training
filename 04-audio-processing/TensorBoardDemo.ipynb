{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Visualization\n",
    "\n",
    "In this part weâ€™ll see how to create a simple but wrong model with Keras, and, gradually, how it can be improved with step-by-step debugging and understanding with TensorBoard.\n",
    "\n",
    "*Please, download [log files](https://drive.google.com/open?id=0B6VSPXOeu5J0TnRhS18xS2pBMFU) first.*\n",
    "\n",
    "Let's start from importing all necessary components/layers for the future CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Convolution2D, BatchNormalization, MaxPooling2D, Flatten, Dense\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import dataset\n",
    "from dataset import MyDataset\n",
    "\n",
    "db=MyDataset(feature_dir=os.path.join('./IRMAS-Sample', 'features', 'Training'), batch_size=8, time_context=128, step=50, \n",
    "             suffix_in='_mel_',suffix_out='_label_',floatX=np.float32,train_percent=0.8)\n",
    "val_data = db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy convolutional model for classification\n",
    "First, we create a skeleton for a model with one convolutional and one dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_classes):\n",
    "\n",
    "    input_shape = (N_MEL_BANDS, SEGMENT_DUR, 1)\n",
    "    channel_axis = 3\n",
    "    melgram_input = Input(shape=input_shape)\n",
    "\n",
    "    m_size = 70\n",
    "    n_size = 3\n",
    "    n_filters = 64\n",
    "    maxpool_const = 4\n",
    "\n",
    "    x = Convolution2D(n_filters, (m_size, n_size),\n",
    "                      padding='same',\n",
    "                      kernel_initializer='zeros',\n",
    "                      kernel_regularizer=l2(1e-5))(melgram_input)\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=(N_MEL_BANDS, SEGMENT_DUR/maxpool_const))(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(n_classes, kernel_initializer='zeros', kernel_regularizer=l2(1e-5), \n",
    "              activation='softmax', name='prediction')(x)\n",
    "\n",
    "    model = Model(melgram_input, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(IRMAS_N_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the model on IRMAS data using the training procedure below.\n",
    "\n",
    "First, we have to define the optimizer. We're using Stochastic Gradient Descent with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 0.001\n",
    "optimizer = SGD(lr=init_lr, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the model structure, specify which metrics we would like to keep eye on and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 96, 128, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 96, 128, 64)       13504     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 96, 128, 64)       256       \n",
      "_________________________________________________________________\n",
      "elu_2 (ELU)                  (None, 96, 128, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 4.0, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256.0)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256.0)             0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 11)                2827      \n",
      "=================================================================\n",
      "Total params: 16,587\n",
      "Trainable params: 16,459\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous part, we have two generators which can provide us training samples and validation samples. \n",
    "We will use them during the training. We also specify the number of steps per epoch, the total number of epoch and the log verbosity level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2289: UserWarning: Expected no kwargs, you passed 1\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/keras/engine/training.py\", line 612, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "TypeError: 'MyDataset' object is not an iterator\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-97590f0d2323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     workers=1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1871\u001b[0m                                          \u001b[0;34m'a tuple `(x, y, sample_weight)` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m                                          \u001b[0;34m'or `(x, y)`. Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1873\u001b[0;31m                                          str(generator_output))\n\u001b[0m\u001b[1;32m   1874\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: None"
     ]
    }
   ],
   "source": [
    "model.fit_generator(db,\n",
    "                    steps_per_epoch=4,\n",
    "                    epochs=4,\n",
    "                    verbose=2,\n",
    "                    validation_data=val_data,\n",
    "                    class_weight=None,\n",
    "                    workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, neither validation nor the training metrics have improved, so we need to explore that's wrong with the model. Keras Callbacks will help us in this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Callbacks\n",
    "\n",
    "The Callback in Keras is a set of functions to be applied to a certain event during the training process.\n",
    "The typical triggers for events are:\n",
    "* on_epoch_begin\n",
    "* on_epoch_end\n",
    "* on_batch_begin\n",
    "* on_batch_end\n",
    "* on_train_begin\n",
    "* on_train_end\n",
    "\n",
    "There are some useful callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_EPOCH)\n",
    "save_clb = ModelCheckpoint(\"{weights_basepath}/\".format(weights_basepath=MODEL_WEIGHT_BASEPATH) +\n",
    "                           \"epoch.{epoch:02d}-val_loss.{val_loss:.3f}\",\n",
    "                           monitor='val_loss',\n",
    "                           save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get acquainted with the TensorBoard Callback.\n",
    "\n",
    "The parameters are:\n",
    "* log_dir - where to store the logs, metadata, and events of the model training process\n",
    "* write_graph - whether or not to write the graph of data and control dependencies\n",
    "* write_grads - whether or not to save the parameters of the model for visualisation\n",
    "* histogram_freq - how often to save the parameters of the model\n",
    "* write_images - whether or not to save the weight and visualise them as images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TensorBoard(log_dir='./example_1',\n",
    "                 write_graph=True, write_grads=True, \n",
    "                 write_images=True, histogram_freq=1)\n",
    "# if we want to compute activations and weight histogram, we need to specify the validation data for that. \n",
    "tb.validation_data = val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the callbacks to the training process and observe the corresponding events and obtain the corresponding logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv2d_2/kernel:0 is illegal; using conv2d_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/kernel:0_grad is illegal; using conv2d_2/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/kernel:0 is illegal; using conv2d_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/bias:0 is illegal; using conv2d_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/bias:0_grad is illegal; using conv2d_2/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/bias:0 is illegal; using conv2d_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/gamma:0 is illegal; using batch_normalization_2/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/gamma:0_grad is illegal; using batch_normalization_2/gamma_0_grad instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/gamma:0 is illegal; using batch_normalization_2/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/beta:0 is illegal; using batch_normalization_2/beta_0 instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/beta:0_grad is illegal; using batch_normalization_2/beta_0_grad instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/beta:0 is illegal; using batch_normalization_2/beta_0 instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/moving_mean:0 is illegal; using batch_normalization_2/moving_mean_0 instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/moving_mean:0_grad is illegal; using batch_normalization_2/moving_mean_0_grad instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/moving_mean:0 is illegal; using batch_normalization_2/moving_mean_0 instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/moving_variance:0 is illegal; using batch_normalization_2/moving_variance_0 instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/moving_variance:0_grad is illegal; using batch_normalization_2/moving_variance_0_grad instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/moving_variance:0 is illegal; using batch_normalization_2/moving_variance_0 instead.\n",
      "INFO:tensorflow:Summary name prediction_1/kernel:0 is illegal; using prediction_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name prediction_1/kernel:0_grad is illegal; using prediction_1/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name prediction_1/kernel:0 is illegal; using prediction_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name prediction_1/bias:0 is illegal; using prediction_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name prediction_1/bias:0_grad is illegal; using prediction_1/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name prediction_1/bias:0 is illegal; using prediction_1/bias_0 instead.\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/keras/engine/training.py\", line 612, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "TypeError: 'MyDataset' object is not an iterator\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a5164c3ffbea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msave_clb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     workers=1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1871\u001b[0m                                          \u001b[0;34m'a tuple `(x, y, sample_weight)` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m                                          \u001b[0;34m'or `(x, y)`. Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1873\u001b[0;31m                                          str(generator_output))\n\u001b[0m\u001b[1;32m   1874\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: None"
     ]
    }
   ],
   "source": [
    "model.fit_generator(db,\n",
    "                    steps_per_epoch=1, # change to STEPS_PER_EPOCH\n",
    "                    epochs=1, # change to MAX_EPOCH_NUM\n",
    "                    verbose=2,\n",
    "                    validation_data=val_data,\n",
    "                    callbacks=[save_clb, early_stopping, tb],\n",
    "                    class_weight=None,\n",
    "                    workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the event files for all runs from [here](https://drive.google.com/open?id=0B6VSPXOeu5J0TnRhS18xS2pBMFU).\n",
    "\n",
    "Now create the `./logs` directory and launch TensorBoard\n",
    "\n",
    "``` bash\n",
    "tar -xvzf logs.tar.gz\n",
    "cd logs\n",
    "tensorboard --logdir ./example_1\n",
    "```\n",
    "\n",
    "and navigate to http://0.0.0.0:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice, that it's almost impossible to see anything on the Graphs tab but we can see vividly that the metrics on the Scalar tab are not improving and the gradients values on the Histograms tab are zero.\n",
    "\n",
    "Our problem is in the weights initialization `kernel_initializer='zeros'` so now we can fix it and define new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e77feff27ae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIRMAS_N_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-e77feff27ae3>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(n_classes)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     x = Dense(n_classes, kernel_initializer='he_normal', kernel_regularizer=l2(1e-5), \n\u001b[0;32m---> 24\u001b[0;31m               activation='softmax', name='prediction')(x)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmelgram_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m    568\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    825\u001b[0m                                       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                                       \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m                                       constraint=self.kernel_constraint)\n\u001b[0m\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             self.bias = self.add_weight(shape=(self.units,),\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/initializers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mstddev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             return K.truncated_normal(shape, 0., stddev,\n\u001b[0;32m--> 204\u001b[0;31m                                       dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mtruncated_normal\u001b[0;34m(shape, mean, stddev, dtype, seed)\u001b[0m\n\u001b[1;32m   3462\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3464\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mtruncated_normal\u001b[0;34m(shape, mean, stddev, dtype, seed, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     rnd = gen_random_ops._truncated_normal(\n\u001b[0;32m--> 172\u001b[0;31m         shape_tensor, dtype, seed=seed1, seed2=seed2)\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0mmul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstddev_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_random_ops.py\u001b[0m in \u001b[0;36m_truncated_normal\u001b[0;34m(shape, dtype, seed, seed2, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \"\"\"\n\u001b[1;32m    335\u001b[0m   result = _op_def_lib.apply_op(\"TruncatedNormal\", shape=shape, dtype=dtype,\n\u001b[0;32m--> 336\u001b[0;31m                                 seed=seed, seed2=seed2, name=name)\n\u001b[0m\u001b[1;32m    337\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    588\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    589\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 61\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"
     ]
    }
   ],
   "source": [
    "def build_model(n_classes):\n",
    "\n",
    "    input_shape = (N_MEL_BANDS, SEGMENT_DUR, 1)\n",
    "    channel_axis = 3\n",
    "    melgram_input = Input(shape=input_shape)\n",
    "\n",
    "    m_size = 70\n",
    "    n_size = 3\n",
    "    n_filters = 64\n",
    "    maxpool_const = 4\n",
    "\n",
    "    x = Convolution2D(n_filters, (m_size, n_size),\n",
    "                      padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=l2(1e-5))(melgram_input)\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=(N_MEL_BANDS, SEGMENT_DUR/maxpool_const))(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(n_classes, kernel_initializer='he_normal', kernel_regularizer=l2(1e-5), \n",
    "              activation='softmax', name='prediction')(x)\n",
    "\n",
    "    model = Model(melgram_input, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(IRMAS_N_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you will repeat the training process, you may notice that classification performance improved significantly.\n",
    "\n",
    "Have a look at a new log file in the `./example_2` directory and restart TensorBoard to explore new data.\n",
    "\n",
    "``` bash\n",
    "cd logs\n",
    "tensorboard --logdir ./example_2 --port=6002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow name scopes\n",
    "\n",
    "You might have noticed the hell on the Graphs tab.\n",
    "That's because TensorBoard can't connect all the data nodes in the model and operations in the training process together, it's smart enough to group the nodes with similar structure but don't expect too much.\n",
    "\n",
    "In order to make the better graph visualisation, we need to define the name scopes for each logical layer and each operation we want to see as an individual element.\n",
    "\n",
    "We can do it just by adding `with tf.name_scope(name_scope)` clause:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2289: UserWarning: Expected no kwargs, you passed 1\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/keras/engine/training.py\", line 612, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "TypeError: 'MyDataset' object is not an iterator\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b02976426d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                         workers=1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1871\u001b[0m                                          \u001b[0;34m'a tuple `(x, y, sample_weight)` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m                                          \u001b[0;34m'or `(x, y)`. Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1873\u001b[0;31m                                          str(generator_output))\n\u001b[0m\u001b[1;32m   1874\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: None"
     ]
    }
   ],
   "source": [
    "global_namescope = 'train'\n",
    "\n",
    "def build_model(n_classes):\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        input_shape = (N_MEL_BANDS, SEGMENT_DUR, 1)\n",
    "        channel_axis = 3\n",
    "        melgram_input = Input(shape=input_shape)\n",
    "\n",
    "        m_size = [5, 5]\n",
    "        n_size = [5, 5]\n",
    "        n_filters = 64\n",
    "        maxpool_const = 8\n",
    "\n",
    "    with tf.name_scope('conv1'):\n",
    "        x = Convolution2D(n_filters, (m_size[0], n_size[0]),\n",
    "                          padding='same',\n",
    "                          kernel_initializer='he_uniform')(melgram_input)\n",
    "        x = BatchNormalization(axis=channel_axis)(x)\n",
    "        x = ELU()(x)\n",
    "        x = MaxPooling2D(pool_size=(maxpool_const, maxpool_const))(x)\n",
    "\n",
    "    with tf.name_scope('conv2'):\n",
    "        x = Convolution2D(n_filters*2, (m_size[1], n_size[1]),\n",
    "                          padding='same',\n",
    "                          kernel_initializer='he_uniform')(x)\n",
    "        x = BatchNormalization(axis=channel_axis)(x)\n",
    "        x = ELU()(x)\n",
    "        x = MaxPooling2D(pool_size=(maxpool_const, maxpool_const))(x)\n",
    "        x = Flatten()(x)\n",
    "\n",
    "    with tf.name_scope('dense1'):\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(n_filters, kernel_initializer='he_uniform', name='hidden')(x)\n",
    "        x = ELU()(x)\n",
    "\n",
    "    with tf.name_scope('dense2'):\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(n_classes, kernel_initializer='he_uniform', activation='softmax', name='prediction')(x)\n",
    "\n",
    "    model = Model(melgram_input, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(IRMAS_N_CLASSES)\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = SGD(lr=init_lr, momentum=0.9, nesterov=True)\n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    model = build_model(IRMAS_N_CLASSES)\n",
    "\n",
    "# for the sake of memory, only graphs now\n",
    "with tf.name_scope('callbacks'):\n",
    "    # The TensorBoard developers are strongly encourage us to use different directories for every run\n",
    "    tb = TensorBoard(log_dir='./example_3', write_graph=True)\n",
    "\n",
    "# yes, we need to recompile the model every time\n",
    "with tf.name_scope('compile'):\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# and preudo-train the model\n",
    "with tf.name_scope(global_namescope):\n",
    "    model.fit_generator(db,\n",
    "                        steps_per_epoch=1, # just one step\n",
    "                        epochs=1, # one epoch to save the graphs\n",
    "                        verbose=2,\n",
    "                        validation_data=val_data,\n",
    "                        callbacks=[tb],\n",
    "                        workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at a new log file in the `./example_3` directory and restart TensorBoard to explore new data.\n",
    "\n",
    "``` bash\n",
    "cd logs\n",
    "tensorboard --logdir ./example_3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and Hidden Layers Output Visualisation\n",
    "\n",
    "With TensorBoard we can also visualise the embeddings of the model. In order to do it, you can add Embedding layer to you model.\n",
    "\n",
    "To visualize the outputs of intermediate layers, we can write our custom callback and use it to store the outputs on validation data during the training process. We will follow the notation from TensorBoard callback, but add some functionality:\n",
    "\n",
    "* layer_names - a list of names of layers to keep eye on\n",
    "* metadata - a path to a TSV file with associated meta information (labels, notes, etc.), [format and details](https://www.tensorflow.org/get_started/embedding_viz) \n",
    "* sprite - a path to a sprite image, [format and details](https://www.tensorflow.org/get_started/embedding_viz) \n",
    "* sprite_shape - a list with values [M, N], the dimensionality of a single image, [format and details](https://www.tensorflow.org/get_started/embedding_viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "if K.backend() == 'tensorflow':\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "class TensorBoardHiddenOutputVis(Callback):\n",
    "    \"\"\"Tensorboard Intermediate Outputs visualization callback.\"\"\"\n",
    "\n",
    "    def __init__(self, log_dir='./logs_embed',\n",
    "                 batch_size=32,\n",
    "                 freq=0,\n",
    "                 layer_names=None,\n",
    "                 metadata=None,\n",
    "                 sprite=None,\n",
    "                 sprite_shape=None):\n",
    "        super(TensorBoardHiddenOutputVis, self).__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.freq = freq\n",
    "        self.layer_names = layer_names\n",
    "        # Notice, that only one file is supported in the present callback\n",
    "        self.metadata = metadata\n",
    "        self.sprite = sprite\n",
    "        self.sprite_shape = sprite_shape\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        self.sess = K.get_session()\n",
    "        self.summary_writer = tf.summary.FileWriter(self.log_dir)\n",
    "        self.outputs_ckpt_path = os.path.join(self.log_dir, 'keras_outputs.ckpt')\n",
    "\n",
    "        if self.freq and self.validation_data:\n",
    "            # define tensors to compute outputs on\n",
    "            outputs_layers = [layer for layer in self.model.layers\n",
    "                                 if layer.name in self.layer_names]\n",
    "            self.output_tensors = [tf.get_default_graph().get_tensor_by_name(layer.get_output_at(0).name)\n",
    "                                   for layer in outputs_layers]\n",
    "\n",
    "            # create configuration for visualisation in the same manner as for embeddings\n",
    "            config = projector.ProjectorConfig()\n",
    "            for i in range(len(self.output_tensors)):\n",
    "                embedding = config.embeddings.add()\n",
    "                embedding.tensor_name = '{ns}/hidden_{i}'.format(ns=global_namescope, i=i)\n",
    "\n",
    "                # Simpliest metadata handler, a single file for all embeddings\n",
    "                if self.metadata:\n",
    "                    embedding.metadata_path = self.metadata\n",
    "\n",
    "                # Sprite image handler\n",
    "                if self.sprite and self.sprite_shape:\n",
    "                    embedding.sprite.image_path = self.sprite\n",
    "                    embedding.sprite.single_image_dim.extend(self.sprite_shape)\n",
    "\n",
    "            # define TF variables to store the hidden outputs during the training\n",
    "            # Notice, that only 1D outputs are supported\n",
    "            self.hidden_vars = [tf.Variable(np.zeros((len(self.validation_data[0]),\n",
    "                                                         self.output_tensors[i].shape[1]),\n",
    "                                                        dtype='float32'),\n",
    "                                               name='hidden_{}'.format(i))\n",
    "                                   for i in range(len(self.output_tensors))]\n",
    "            # add TF variables into computational graph\n",
    "            for hidden_var in self.hidden_vars:\n",
    "                self.sess.run(hidden_var.initializer)\n",
    "\n",
    "            # save the config and setup TF saver for hidden variables\n",
    "            projector.visualize_embeddings(self.summary_writer, config)\n",
    "            self.saver = tf.train.Saver(self.hidden_vars)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data and self.freq:\n",
    "            if epoch % self.freq == 0:\n",
    "\n",
    "                val_data = self.validation_data\n",
    "                tensors = (self.model.inputs +\n",
    "                           self.model.targets +\n",
    "                           self.model.sample_weights)\n",
    "                all_outputs = [[]]*len(self.output_tensors)\n",
    "\n",
    "                if self.model.uses_learning_phase:\n",
    "                    tensors += [K.learning_phase()]\n",
    "\n",
    "                assert len(val_data) == len(tensors)\n",
    "                val_size = val_data[0].shape[0]\n",
    "                i = 0\n",
    "                # compute outputs batch by batch on validation data\n",
    "                while i < val_size:\n",
    "                    step = min(self.batch_size, val_size - i)\n",
    "                    batch_val = []\n",
    "                    batch_val.append(val_data[0][i:i + step])\n",
    "                    batch_val.append(val_data[1][i:i + step])\n",
    "                    batch_val.append(val_data[2][i:i + step])\n",
    "                    if self.model.uses_learning_phase:\n",
    "                        batch_val.append(val_data[3])\n",
    "                    feed_dict = dict(zip(tensors, batch_val))\n",
    "                    tensor_outputs = self.sess.run(self.output_tensors, feed_dict=feed_dict)\n",
    "                    for output_idx, tensor_output in enumerate(tensor_outputs):\n",
    "                        all_outputs[output_idx].extend(tensor_output)\n",
    "                    i += self.batch_size\n",
    "                \n",
    "                # rewrite the current state of hidden outputs with new values\n",
    "                for idx, embed in enumerate(self.hidden_vars):\n",
    "                    embed.assign(np.array(all_outputs[idx])).eval(session=self.sess)\n",
    "                self.saver.save(self.sess, self.outputs_ckpt_path, epoch)\n",
    "\n",
    "        self.summary_writer.flush()\n",
    "\n",
    "    def on_train_end(self, _):\n",
    "        self.summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the new callback, recompile and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_monitor = ['hidden']\n",
    "# find the files precomputed in ./logs_embed directory \n",
    "metadata_file_name = 'metadata.tsv'\n",
    "sprite_file_name = 'sprite.png'\n",
    "sprite_shape = [N_MEL_BANDS, SEGMENT_DUR]\n",
    "\n",
    "with tf.name_scope('callbacks'):\n",
    "    tbe = TensorBoardHiddenOutputVis(log_dir='./logs_embed', freq=1,\n",
    "                           layer_names=layers_to_monitor,\n",
    "                           metadata=metadata_file_name,\n",
    "                           sprite=sprite_file_name,\n",
    "                           sprite_shape=sprite_shape)\n",
    "    tbe.validation_data = val_data\n",
    "\n",
    "with tf.name_scope('compile'):\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "with tf.name_scope(global_namescope):\n",
    "    model.fit_generator(db,\n",
    "                        steps_per_epoch=1, # change to STEPS_PER_EPOCH\n",
    "                        epochs=1, # change to MAX_EPOCH_NUM\n",
    "                        verbose=2,\n",
    "                        callbacks=[tbe],\n",
    "                        validation_data=val_data,\n",
    "                        class_weight=None,\n",
    "                        workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of time, we're going to skip the training process. You can find the corresponding data in `./logs_embed` directory.\n",
    "\n",
    "Restart TensorBoard, navigate to http://0.0.0.0:6006/ and now we can discuss the visualisation.\n",
    "``` bash\n",
    "cd logs\n",
    "tensorboard --logdir ./logs_embed\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
